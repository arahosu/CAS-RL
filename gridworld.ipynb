{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld Environment\n",
    "\n",
    "In this tutorial, we’ll explore some ideas in reinforcement learning by using a simple example called GridWorld. In this environment, an “agent” (imagine it as a character) learns how to move through a grid to reach certain goals. This setup will help us understand a mathematical framework called a Markov Decision Process (MDP), which underpins many AI systems that make decisions.\n",
    "\n",
    "## Markov Decision Processes\n",
    "\n",
    "![title](figures/MDP-model.png)\n",
    "\n",
    "Before jumping into the math, let’s break down the idea of a Markov Decision Process.\n",
    "Imagine you’re in a maze. You start somewhere in the maze and can take actions (like moving forward or turning left) that change your position. At each point, you might receive a reward, or a penalty (if, for instance, you'd find a prize or fall into a trap). Over time, you want to learn how to move through the maze in a way that maximizes your rewards. Importantly, the outcome of a given action is only impacted by that given action and the current state when you took the action - it is not impacted by the previous history of actions you have taken. th This process, of making decisions based on where you are and what actions are available, is what a Markov Decision Process describes.\n",
    "Now let’s look at the formal definition.\n",
    "MDP Components\n",
    "An MDP is defined by a tuple: $(S, A, P, R, \\gamma)$. A tuple just means that we’re listing a set of related things that describe our situation. Here’s what each part means:\n",
    "- $S$ (Set of States): Think of states as all the possible situations or places the agent can be in. For GridWorld, each square in the grid is a unique state.\n",
    "- $A$ (Set of Actions): These are the choices the agent has at each state. For example, in GridWorld, the actions might be moving Up, Down, Left, or Right.\n",
    "- $P(s'|s,a)$ (Transition Probability): This part tells us the likelihood of ending up in a new state, $s'$, if we’re in state $s$ and take action $a$. For example, if the agent moves “Up” from a particular square, it might reach the square directly above it with 80% certainty, but sometimes it might veer off in another direction.\n",
    "- $R(s,a,s')$ (Reward): This represents the reward (a positive or negative value) that the agent receives for taking action $a$ in state $s$ and ending up in state $s'$. Rewards are like points: some states might give a big positive reward (like reaching the goal), while others give penalties (like stepping into a pit).\n",
    "- $\\gamma$ (Discount Factor): The discount factor $\\gamma$ is a number between 0 and 1 that determines how much importance we give to future rewards. If $\\gamma$ is closer to 1, the agent cares more about long-term rewards. If it’s closer to 0, it focuses more on immediate rewards.\n",
    "\n",
    "## Markov Property\n",
    "One last piece of the puzzle is the Markov property. This is a special rule that simplifies our calculations: in an MDP, the next state and reward depend only on the current state and action. The history of past states and actions doesn’t matter.\n",
    "This is why we can represent the MDP with only a current state and action, rather than keeping track of the entire sequence of actions that led us there.\n",
    "\n",
    "## Applying MDPs to GridWorld\n",
    "\n",
    "![title](figures/gridworld_sample.png)\n",
    "\n",
    "To make this clearer, let’s see how these concepts apply to our GridWorld example:\n",
    "- States ($S$): Each square in the grid is a unique state.\n",
    "- Actions ($A$): The agent can choose to move Up, Down, Left, or Right.\n",
    "- Transitions ($P$): There’s a bit of randomness in the agent’s actions. For example, if it tries to go “Up,” it might end up moving “Left” or “Right” by mistake. We control this randomness with a “noise” parameter.\n",
    "- Rewards ($R$):\n",
    "    - Reaching the goal square: +10 points\n",
    "    - Stepping into a pit: -10 points\n",
    "    - Moving to any other square: -1 point (a small penalty to encourage finding the shortest path)\n",
    "- $\\gamma$: We can adjust this to see how it affects the agent’s choices between immediate rewards and future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GridWorld\n",
    "\n",
    "Run the code cells below to set up the GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size=4, seed=None, noise=0.1):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.size = size\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.grid = np.zeros((size, size), dtype=int)\n",
    "        self._generate_grid()\n",
    "        self.start_pos = (0, 0)\n",
    "        self.agent_pos = self.start_pos\n",
    "        \n",
    "        # Stochastic transition parameters\n",
    "        self.noise = noise  # Probability of taking a random action instead of chosen action\n",
    "        \n",
    "        self.step_cost = -1.\n",
    "        self.terminal_rewards = {\n",
    "            2: 10.0,    # goal reward\n",
    "            -1: -10.0   # pit reward\n",
    "        }\n",
    "        \n",
    "        # Action mappings\n",
    "        self.actions = {\n",
    "            0: (0, 1),   # right\n",
    "            1: (1, 0),   # down\n",
    "            2: (0, -1),  # left\n",
    "            3: (-1, 0)   # up\n",
    "        }\n",
    "        \n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"Check if state is terminal\"\"\"\n",
    "        r, c = state\n",
    "        return self.grid[r, c] in [2, -1]  # goal or pit\n",
    "        \n",
    "    def get_terminal_value(self, state):\n",
    "        \"\"\"Get the value for terminal state\"\"\"\n",
    "        r, c = state\n",
    "        return self.terminal_rewards[self.grid[r, c]]\n",
    "        \n",
    "    def _get_reachable_cells(self, start, grid):\n",
    "        \"\"\"Get all reachable non-wall, non-terminal cells from start position using BFS\"\"\"\n",
    "        visited = set()\n",
    "        queue = deque([start])\n",
    "        visited.add(start)\n",
    "        \n",
    "        while queue:\n",
    "            r, c = queue.popleft()\n",
    "            for dr, dc in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n",
    "                new_r, new_c = r + dr, c + dc\n",
    "                if (0 <= new_r < self.size and \n",
    "                    0 <= new_c < self.size and \n",
    "                    grid[new_r, new_c] not in [1, 2, -1] and  # Not wall or terminal\n",
    "                    (new_r, new_c) not in visited):\n",
    "                    queue.append((new_r, new_c))\n",
    "                    visited.add((new_r, new_c))\n",
    "        return visited\n",
    "        \n",
    "    def _generate_grid(self):\n",
    "        \"\"\"Generate grid ensuring all non-terminal states are reachable\"\"\"\n",
    "        while True:\n",
    "            # Reset grid\n",
    "            self.grid.fill(0)\n",
    "            \n",
    "            # Place walls first (20% chance)\n",
    "            for i in range(self.size):\n",
    "                for j in range(self.size):\n",
    "                    if (i, j) != (0, 0) and self.rng.random() < 0.2:\n",
    "                        self.grid[i, j] = 1\n",
    "            \n",
    "            # Get all non-wall cells\n",
    "            non_wall_cells = set((i, j) for i in range(self.size) \n",
    "                               for j in range(self.size) \n",
    "                               if self.grid[i, j] != 1)\n",
    "            \n",
    "            # Check initial reachability\n",
    "            reachable = self._get_reachable_cells((0, 0), self.grid)\n",
    "            if len(reachable) != len(non_wall_cells):\n",
    "                continue\n",
    "            \n",
    "            # Remove start position from candidate positions\n",
    "            non_wall_cells.remove((0, 0))\n",
    "            candidates = list(non_wall_cells)\n",
    "            \n",
    "            if len(candidates) < 2:  # Need at least space for goal and pit\n",
    "                continue\n",
    "                \n",
    "            # Place goal in a reachable cell (preferably far from start)\n",
    "            distances = [(abs(r-0) + abs(c-0), (r, c)) for r, c in candidates]\n",
    "            distances.sort(reverse=True)\n",
    "            goal_pos = distances[0][1]\n",
    "            self.grid[goal_pos] = 2\n",
    "            candidates.remove(goal_pos)\n",
    "            \n",
    "            # Place pits\n",
    "            num_pits = max(1, len(candidates) // 10)\n",
    "            pit_candidates = []\n",
    "            \n",
    "            # Try placing each pit and check reachability\n",
    "            for _ in range(num_pits):\n",
    "                if not candidates:\n",
    "                    break\n",
    "                    \n",
    "                valid_pit_placed = False\n",
    "                np.random.shuffle(candidates)  # Randomize order\n",
    "                \n",
    "                for pit_pos in candidates[:]:  # Use copy for iteration\n",
    "                    # Temporarily place pit\n",
    "                    self.grid[pit_pos] = -1\n",
    "                    \n",
    "                    # Get non-terminal, non-wall states\n",
    "                    non_terminal_cells = set((i, j) for i in range(self.size) \n",
    "                                          for j in range(self.size) \n",
    "                                          if self.grid[i, j] not in [1, 2, -1])\n",
    "                    \n",
    "                    # Check if all non-terminal states are still reachable\n",
    "                    reachable = self._get_reachable_cells((0, 0), self.grid)\n",
    "                    \n",
    "                    if len(reachable) == len(non_terminal_cells):\n",
    "                        valid_pit_placed = True\n",
    "                        candidates.remove(pit_pos)\n",
    "                        pit_candidates.append(pit_pos)\n",
    "                        break\n",
    "                    else:\n",
    "                        # Remove invalid pit placement\n",
    "                        self.grid[pit_pos] = 0\n",
    "                \n",
    "                if not valid_pit_placed:\n",
    "                    break\n",
    "            \n",
    "            # If we placed at least one pit successfully, accept the grid\n",
    "            if pit_candidates:\n",
    "                break\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = self.start_pos\n",
    "        return self.agent_pos\n",
    "\n",
    "    def get_transition_probs(self, state, action):\n",
    "        \"\"\"\n",
    "        Get transition probabilities for a given state-action pair.\n",
    "        Returns a list of (next_state, probability) tuples.\n",
    "        \"\"\"\n",
    "        if self.noise == 0.:\n",
    "            next_state = self._get_next_state(state, action)\n",
    "            return [(next_state, 1.0)]\n",
    "            \n",
    "        transitions = []\n",
    "        # Intended action\n",
    "        main_prob = 1.0 - self.noise\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        transitions.append((next_state, main_prob))\n",
    "        \n",
    "        # Random actions due to noise\n",
    "        noise_prob = self.noise / 3  # Split noise probability among other actions\n",
    "        for a in range(4):\n",
    "            if a != action:\n",
    "                next_state = self._get_next_state(state, a)\n",
    "                transitions.append((next_state, noise_prob))\n",
    "                \n",
    "        return transitions\n",
    "    \n",
    "    def _get_next_state(self, state, action):\n",
    "        \"\"\"Helper method to compute next state given current state and action\"\"\"\n",
    "        r, c = state\n",
    "        dr, dc = self.actions[action]\n",
    "        new_r = max(0, min(r + dr, self.size - 1))\n",
    "        new_c = max(0, min(c + dc, self.size - 1))\n",
    "        \n",
    "        # If wall, stay in current position\n",
    "        if self.grid[new_r, new_c] == 1:\n",
    "            return (r, c)\n",
    "        return (new_r, new_c)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment.\n",
    "        Returns (next_state, reward, done)\n",
    "        \"\"\"\n",
    "        if self.rng.random() < self.noise:\n",
    "            # With probability noise, take a random action instead\n",
    "            other_actions = [a for a in range(4) if a != action]\n",
    "            action = self.rng.choice(other_actions)\n",
    "        \n",
    "        next_pos = self._get_next_state(self.agent_pos, action)\n",
    "        self.agent_pos = next_pos\n",
    "        \n",
    "        # Get state type at new position\n",
    "        cell_type = self.grid[self.agent_pos]\n",
    "        \n",
    "        if self.is_terminal(self.agent_pos):\n",
    "            reward = self.terminal_rewards[cell_type]\n",
    "            done = True\n",
    "        else:\n",
    "            reward = self.step_cost\n",
    "            done = False\n",
    "            \n",
    "        return self.agent_pos, reward, done\n",
    "\n",
    "    def render(self, ax=None):\n",
    "        \"\"\"\n",
    "        Render the GridWorld using Unicode characters and consistent colors\n",
    "        \n",
    "        Args:\n",
    "            ax: matplotlib axes to render on. If None, creates new figure\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            ax = plt.gca()\n",
    "            \n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Define cell styles using Unicode characters\n",
    "        cell_styles = {\n",
    "            0: {'color': 'white', 'symbol': None},       # empty cell\n",
    "            1: {'color': 'gray', 'symbol': '■'},         # wall\n",
    "            2: {'color': 'lightgreen', 'symbol': '⚑'},   # goal\n",
    "            -1: {'color': 'pink', 'symbol': '☠'}         # pit\n",
    "        }\n",
    "        \n",
    "        # Draw grid cells\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                cell_type = self.grid[i, j]\n",
    "                style = cell_styles[cell_type]\n",
    "                \n",
    "                # Determine cell color\n",
    "                if (i, j) == self.start_pos:\n",
    "                    cell_color = 'lightblue'  # Blue for start position\n",
    "                else:\n",
    "                    cell_color = style['color']\n",
    "                    \n",
    "                # Fill cell with color\n",
    "                ax.fill([j, j+1, j+1, j], [i, i, i+1, i+1], cell_color)\n",
    "        \n",
    "        # Draw start position\n",
    "        sr, sc = self.start_pos\n",
    "        ax.text(sc + 0.5, sr + 0.5, '▶',\n",
    "            ha='center', va='center', fontsize=20)\n",
    "        \n",
    "        ax.set_xlim(0, self.size)\n",
    "        ax.set_ylim(self.size, 0)\n",
    "        ax.set_title('GridWorld')\n",
    "        \n",
    "        if ax is None:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "\n",
    "def create_interactive_visualization(env_class, func, theta=1e-9, draw_agent=True):\n",
    "    \"\"\"\n",
    "    Create streamlined interactive visualization for a planning algorithm.\n",
    "    \"\"\"\n",
    "    # Create widgets\n",
    "    seed_dropdown = widgets.Dropdown(\n",
    "        options=[('Grid 1', 0), ('Grid 2', 1), ('Grid 3', 21)],\n",
    "        value=0,\n",
    "        description='Grid:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    gamma_slider = widgets.FloatSlider(\n",
    "        value=0.95,\n",
    "        min=0.5,\n",
    "        max=0.99,\n",
    "        step=0.01,\n",
    "        description='Gamma:',\n",
    "        style={'description_width': 'initial'},\n",
    "        readout_format='.2f'\n",
    "    )\n",
    "    \n",
    "    noise_slider = widgets.FloatSlider(\n",
    "        value=0.,\n",
    "        min=0.0,\n",
    "        max=0.4,\n",
    "        step=0.05,\n",
    "        description='Noise:',\n",
    "        style={'description_width': 'initial'},\n",
    "        readout_format='.2f'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def update_plots(*args):\n",
    "        with output:\n",
    "            output.clear_output(wait=True)\n",
    "            \n",
    "            try:\n",
    "                # Create new environment with current parameters\n",
    "                env = env_class(\n",
    "                    size=5, \n",
    "                    seed=seed_dropdown.value,\n",
    "                    noise=noise_slider.value\n",
    "                )\n",
    "                \n",
    "                # Create figure with two subplots\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                \n",
    "                # First subplot: Original grid\n",
    "                env.render(ax=ax1)\n",
    "                \n",
    "                # Second subplot: Values and policy\n",
    "                V, policy = func(env, gamma=gamma_slider.value, theta=theta)\n",
    "                plot_values_and_policy(V, policy, env, \n",
    "                                     f\"{func.__name__}\\nγ={gamma_slider.value:.2f}, noise={noise_slider.value:.2f}\",\n",
    "                                     ax=ax2)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Connect widget changes to update function\n",
    "    seed_dropdown.observe(update_plots, 'value')\n",
    "    gamma_slider.observe(update_plots, 'value')\n",
    "    noise_slider.observe(update_plots, 'value')\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.HBox([\n",
    "        seed_dropdown,\n",
    "        gamma_slider,\n",
    "        noise_slider\n",
    "    ])\n",
    "    \n",
    "    # Display widgets and output\n",
    "    display(controls)\n",
    "    display(output)\n",
    "    \n",
    "    # Initial plot\n",
    "    update_plots()\n",
    "\n",
    "def create_qlearning_visualization(env_class, q_learning_func):\n",
    "    \"\"\"\n",
    "    Create interactive visualization for Q-learning that uses an externally defined Q-learning function.\n",
    "    \n",
    "    Args:\n",
    "        env_class: The environment class (GridWorld)\n",
    "        q_learning_func: Function that implements Q-learning algorithm\n",
    "            Expected signature: \n",
    "            q_learning_func(env, episodes, alpha, gamma, epsilon) -> (Q, rewards, V, policy)\n",
    "    \"\"\"\n",
    "    # Create widgets\n",
    "    seed_dropdown = widgets.Dropdown(\n",
    "        options=[('Grid 1', 0), ('Grid 2', 1), ('Grid 3', 21)],\n",
    "        value=0,\n",
    "        description='Grid:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    alpha_slider = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0.01,\n",
    "        max=0.5,\n",
    "        step=0.01,\n",
    "        description='Learning Rate:',\n",
    "        style={'description_width': 'initial'},\n",
    "        readout_format='.2f'\n",
    "    )\n",
    "    \n",
    "    gamma_slider = widgets.FloatSlider(\n",
    "        value=0.95,\n",
    "        min=0.5,\n",
    "        max=0.99,\n",
    "        step=0.01,\n",
    "        description='Gamma:',\n",
    "        style={'description_width': 'initial'},\n",
    "        readout_format='.2f'\n",
    "    )\n",
    "    \n",
    "    noise_slider = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0.0,\n",
    "        max=0.4,\n",
    "        step=0.05,\n",
    "        description='Noise:',\n",
    "        style={'description_width': 'initial'},\n",
    "        readout_format='.2f'\n",
    "    )\n",
    "    \n",
    "    episodes_slider = widgets.IntSlider(\n",
    "        value=50000,\n",
    "        min=10000,\n",
    "        max=100000,\n",
    "        step=10000,\n",
    "        description='Episodes:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    epsilon_slider = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0.01,\n",
    "        max=0.5,\n",
    "        step=0.01,\n",
    "        description='Epsilon:',\n",
    "        style={'description_width': 'initial'},\n",
    "        readout_format='.2f'\n",
    "    )\n",
    "    \n",
    "    run_button = widgets.Button(\n",
    "        description='Run Q-Learning',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def run_qlearning(b):\n",
    "        \"\"\"Run Q-learning with current parameters and update plots\"\"\"\n",
    "        with output:\n",
    "            output.clear_output(wait=True)\n",
    "            \n",
    "            try:\n",
    "                # Create environment\n",
    "                env = env_class(\n",
    "                    size=5, \n",
    "                    seed=seed_dropdown.value,\n",
    "                    noise=noise_slider.value\n",
    "                )\n",
    "                \n",
    "                # Run Q-learning using provided function\n",
    "                Q, rewards, V, policy = q_learning_func(\n",
    "                    env,\n",
    "                    episodes=episodes_slider.value,\n",
    "                    alpha=alpha_slider.value,\n",
    "                    gamma=gamma_slider.value,\n",
    "                    epsilon=epsilon_slider.value\n",
    "                )\n",
    "                \n",
    "                # Create plots\n",
    "                fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "                \n",
    "                # Plot original grid\n",
    "                env.render(ax=ax1)\n",
    "                \n",
    "                # Plot training progress\n",
    "                window_size = 100\n",
    "                moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "                ax2.plot(rewards, alpha=0.3, color='blue', label='Episode Rewards')\n",
    "                ax2.plot(range(window_size-1, len(rewards)), moving_avg, \n",
    "                        color='blue', label=f'{window_size}-Episode Moving Average')\n",
    "                ax2.set_xlabel('Episode')\n",
    "                ax2.set_ylabel('Reward')\n",
    "                ax2.legend()\n",
    "                ax2.set_title('Training Progress')\n",
    "                \n",
    "                # Plot learned values and policy with adjusted color scale\n",
    "                sns.heatmap(V, annot=True, fmt='.1f', cmap='RdYlGn',\n",
    "                          cbar_kws={'label': 'Value'}, ax=ax3, square=True,\n",
    "                          mask=V == 0, vmin=-10, vmax=10)\n",
    "                \n",
    "                # Add arrows for policy\n",
    "                for i in range(env.size):\n",
    "                    for j in range(env.size):\n",
    "                        if env.grid[i, j] in [1, 2, -1]:  # Skip walls and terminal states\n",
    "                            continue\n",
    "                        \n",
    "                        action = policy[i, j]\n",
    "                        if action == 0:    # right\n",
    "                            ax3.arrow(j + 0.5, i + 0.5, 0.3, 0, head_width=0.1, color='black')\n",
    "                        elif action == 1:  # down\n",
    "                            ax3.arrow(j + 0.5, i + 0.5, 0, 0.3, head_width=0.1, color='black')\n",
    "                        elif action == 2:  # left\n",
    "                            ax3.arrow(j + 0.5, i + 0.5, -0.3, 0, head_width=0.1, color='black')\n",
    "                        elif action == 3:  # up\n",
    "                            ax3.arrow(j + 0.5, i + 0.5, 0, -0.3, head_width=0.1, color='black')\n",
    "                \n",
    "                ax3.set_title(f\"Q-Learning Results\\nα={alpha_slider.value:.2f}, γ={gamma_slider.value:.2f}, ε={epsilon_slider.value:.2f}\")\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Connect run button to function\n",
    "    run_button.on_click(run_qlearning)\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HBox([seed_dropdown, alpha_slider, gamma_slider]),\n",
    "        widgets.HBox([noise_slider, episodes_slider, epsilon_slider]),\n",
    "        run_button\n",
    "    ])\n",
    "    \n",
    "    # Display widgets and output\n",
    "    display(controls)\n",
    "    display(output)\n",
    "\n",
    "\n",
    "def plot_values_and_policy(V, policy, env, title, ax=None):\n",
    "    \"\"\"Plot values and policy arrows on a heatmap.\"\"\"\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    # Plot values as heatmap\n",
    "    \n",
    "    sns.heatmap(V, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                cbar_kws={'label': 'Value'}, ax=ax, square=True, mask = V == 0.)\n",
    "    \n",
    "    # Add arrows for policy\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if env.grid[i, j] in [1, 2, -1]:  # Skip walls and terminal states\n",
    "                continue\n",
    "            \n",
    "            action = policy[i, j]\n",
    "            if action == 0:    # right\n",
    "                ax.arrow(j + 0.5, i + 0.5, 0.3, 0, head_width=0.1, color='black')\n",
    "            elif action == 1:  # down\n",
    "                ax.arrow(j + 0.5, i + 0.5, 0, 0.3, head_width=0.1, color='black')\n",
    "            elif action == 2:  # left\n",
    "                ax.arrow(j + 0.5, i + 0.5, -0.3, 0, head_width=0.1, color='black')\n",
    "            elif action == 3:  # up\n",
    "                ax.arrow(j + 0.5, i + 0.5, 0, -0.3, head_width=0.1, color='black')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Returns and Value Functions\n",
    "\n",
    "In reinforcement learning, our goal is to train an **agent** to make decisions that lead to high rewards over time. However, rewards may not all come at once, and they might be spread across future steps. To help balance short-term and long-term rewards, we use something called **discounted returns**. Let’s go over this concept carefully.\n",
    "\n",
    "### Understanding Discounted Returns\n",
    "\n",
    "Imagine that you’re earning rewards as you go through different steps in an environment (like moving through a maze). Let’s say each step from time $t$ onward has a reward associated with it: $\\{r_t, r_{t+1}, r_{t+2}, ...\\}$. But as time passes, these rewards in the future might not be as valuable to us as immediate rewards. This is where the **discount factor**, $\\gamma$, comes in.\n",
    "\n",
    "The **discounted return** starting at a specific time step $t$, denoted $G_t$, is a way of adding up all the future rewards we expect to get, but with each one being worth slightly less as it gets further in the future. The formula for this is:\n",
    "\n",
    "$ G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} $\n",
    "\n",
    "Here:\n",
    "- $G_t$ is the **discounted return** starting from time step $t$.\n",
    "- $\\gamma$ is the **discount factor**, a number between 0 and 1.\n",
    "- $r_{t+k}$ is the reward received $k$ steps after time $t$.\n",
    "\n",
    "### Why Do We Use a Discount Factor?\n",
    "\n",
    "The discount factor, $\\gamma$, has two important roles:\n",
    "1. **Prioritizing Immediate Rewards**: If $\\gamma$ is close to 1, the agent will consider future rewards almost as important as immediate rewards. If $\\gamma$ is closer to 0, the agent will focus more on immediate rewards and almost ignore distant rewards.\n",
    "2. **Making the Sum Finite**: When we add up rewards into the distant future, $\\gamma$ helps ensure that the total doesn’t become infinitely large. For example, if there were rewards at every step and no discounting, the sum could grow indefinitely. By discounting future rewards, we keep this sum finite, even if we consider rewards stretching infinitely into the future.\n",
    "\n",
    "### Policies and Their Goal\n",
    "\n",
    "A **policy** is a rule or strategy that tells the agent what action to take in each state. We denote a policy as $\\pi$, and it can either be:\n",
    "- **Deterministic**: Always chooses the same action for a given state.\n",
    "- **Stochastic**: Randomly chooses actions based on a set of probabilities.\n",
    "\n",
    "The ultimate goal in reinforcement learning is to find an **optimal policy**, $\\pi^*$, that maximizes the expected discounted return $G_t$ from any starting state. In other words, we want to find a way for the agent to consistently choose actions that yield the highest possible cumulative reward over time.\n",
    "\n",
    "### Value Functions: Quantifying Good Choices\n",
    "\n",
    "To help find this optimal policy, we introduce two types of functions, called **value functions**, that estimate how good it is to be in a particular state or to take a particular action from that state.\n",
    "\n",
    "#### 1. State-Value Function ($V^\\pi(s)$)\n",
    "\n",
    "The **state-value function** is a way of estimating the expected return (total future rewards) if the agent starts from a specific state $s$ and follows a particular policy $\\pi$. We denote this as $V^\\pi(s)$.\n",
    "\n",
    "Formally, we write:\n",
    "\n",
    "$ V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s] $\n",
    "\n",
    "In words:\n",
    "- $V^\\pi(s)$ is the **expected discounted return** starting from state $s$ and following policy $\\pi$.\n",
    "- This expectation $\\mathbb{E}_\\pi$ means we’re averaging over the possible outcomes (since the rewards might vary depending on the environment’s randomness and the policy).\n",
    "\n",
    "When we find the **optimal policy**, $\\pi^*$, we get the **optimal state-value function** $V^*(s)$, which tells us the maximum possible return we can expect from each state. This optimal state-value function satisfies the following equation:\n",
    "\n",
    "$ V^*(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^*(s')] $\n",
    "\n",
    "Breaking it down:\n",
    "- $P(s'|s,a)$ is the probability of ending up in state $s'$ after taking action $a$ in state $s$.\n",
    "- $R(s,a,s')$ is the reward received for taking action $a$ in state $s$ and ending up in state $s'$.\n",
    "- $\\gamma V^*(s')$ is the discounted future value, given by the optimal value function for the next state $s'$.\n",
    "\n",
    "The term $\\max_a$ means we’re choosing the action $a$ that maximizes our expected return from state $s$.\n",
    "\n",
    "#### 2. Action-Value Function ($Q^\\pi(s,a)$)\n",
    "\n",
    "The **action-value function** estimates the expected return if the agent starts in state $s$, takes action $a$, and then follows policy $\\pi$. This is written as $Q^\\pi(s, a)$.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$ Q^\\pi(s,a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] $\n",
    "\n",
    "Here, $Q^\\pi(s,a)$ represents the **expected discounted return** starting from state $s$, taking action $a$, and then following policy $\\pi$.\n",
    "\n",
    "When we find the optimal policy, we get the **optimal action-value function** $Q^*(s, a)$, which represents the maximum expected return starting from $s$ and taking action $a$. It satisfies the following equation:\n",
    "\n",
    "$ Q^*(s, a) = \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma \\max_{a'} Q^*(s', a')] $\n",
    "\n",
    "Breaking it down:\n",
    "- The **sum** is over all possible next states $s'$.\n",
    "- $P(s'|s,a)$ is the probability of reaching state $s'$ from state $s$ by taking action $a$.\n",
    "- The term $\\max_{a'} Q^*(s', a')$ tells us the maximum expected return from the next state $s'$, assuming we act optimally from there onward.\n",
    "\n",
    "### Why We Need Iterative Methods\n",
    "\n",
    "Finding the exact values of these optimal functions, $V^*(s)$ and $Q^*(s, a)$, can be challenging due to:\n",
    "1. The **stochastic nature of transitions** (the randomness in outcomes).\n",
    "2. The **recursive structure** of the Bellman equations (where each state’s value depends on the values of future states).\n",
    "3. The **max operator**, which adds complexity in calculating the best action at each state.\n",
    "\n",
    "Because of these challenges, we use **iterative methods** to approximate the optimal values and policies:\n",
    "1. **Value Iteration**: This method directly finds $V^*$ by repeatedly updating estimates of state values.\n",
    "2. **Policy Iteration**: This approach alternates between evaluating a policy (estimating its value function) and improving the policy based on those estimates.\n",
    "3. **Q-Learning**: This method learns $Q^*$ by exploring the environment and updating action-value estimates without requiring knowledge of $P$ (transition probabilities) and $R$ (rewards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "**Value Iteration** is a method for finding the optimal value function in a Markov Decision Process (MDP) by repeatedly updating value estimates for each state until they converge to the optimal values. This process is based on something called the **Bellman optimality equation**. \n",
    "\n",
    "### The Value Iteration Update Rule\n",
    "\n",
    "Value Iteration begins with an arbitrary initial guess for the value function, often denoted $V_0$. Then, it continuously improves these value estimates by updating each state $s$ according to the following formula:\n",
    "\n",
    "$ V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V_k(s') \\right] $\n",
    "\n",
    "Let’s break down each component of this update rule:\n",
    "\n",
    "1. **Immediate Reward ($R(s,a,s')$)**: This term represents the reward we receive immediately when we move from state $s$ to state $s'$ by taking action $a$.\n",
    "\n",
    "2. **Discounted Future Value ($\\gamma V_k(s')$)**: This part takes into account the expected future rewards, starting from the next state $s'$. The discount factor $\\gamma$ (where $0 \\leq \\gamma \\leq 1$) makes future rewards slightly less valuable than immediate ones.\n",
    "\n",
    "3. **Expectation Over Next States ($\\sum_{s'} P(s'|s,a)[\\cdot]$)**: Since actions can lead to multiple possible next states, we use the probabilities $P(s'|s,a)$ to calculate a weighted average. This accounts for the chance of ending up in each potential next state $s'$ from state $s$ after taking action $a$.\n",
    "\n",
    "4. **Maximizing Over Actions ($\\max_a$)**: Finally, we choose the action $a$ that maximizes the expected reward. This ensures that each state value $V(s)$ represents the maximum achievable return starting from that state, assuming we always act optimally.\n",
    "\n",
    "### Convergence and the Optimal Policy\n",
    "\n",
    "Value Iteration continues updating the values of all states until the values converge, meaning the values no longer change significantly between updates. When the difference in values across all states is smaller than a small threshold $\\theta$, we consider the values to have **converged**. At this point, we can determine the **optimal policy** $\\pi^*$ by choosing the action that gives the highest expected reward for each state $s$:\n",
    "\n",
    "$\\pi^*(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V^*(s') \\right]$\n",
    "\n",
    "### Value Iteration Pseudocode\n",
    "\n",
    "The following pseudocode summarizes the Value Iteration process:\n",
    "\n",
    "**Input**: MDP $(S, A, P, R)$, discount factor $\\gamma$, tolerance $\\theta$  \n",
    "**Output**: Value function $V$, policy $\\pi$\n",
    "\n",
    "1. **Initialize**: Set $V(s) \\leftarrow 0$ for all $s \\in S$\n",
    "2. **Repeat**:\n",
    "    - Set $\\Delta \\leftarrow 0$ (to track the maximum change in values for this iteration)\n",
    "    - **For each** state $s \\in S$:\n",
    "        - Store the current value: $v \\leftarrow V(s)$\n",
    "        - Update the value function using the Bellman optimality equation:\n",
    "          $V(s) \\leftarrow \\max_a \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V(s') \\right]$\n",
    "        - Update $\\Delta$: $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$\n",
    "3. **Until** $\\Delta < \\theta$ (indicating convergence)\n",
    "4. **Extract Policy**: For each state, define the policy as:\n",
    "   $\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V(s') \\right]$\n",
    "\n",
    "This procedure yields both the optimal value function $V^*$ and the optimal policy $\\pi^*$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Initialize Value (V) and Policy (policy)\n",
    "- Skip Walls and Terminal States from the calculation\n",
    "- Compute the expected value in both cases where state is terminal, and when it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bc725d1aef4e229cb50a41b1083df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Grid:', options=(('Grid 1', 0), ('Grid 2', 1), ('Grid 3', 21)), style=Des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2692ba543c4dbda38ee39000e862da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"\"\n",
    "    Value Iteration algorithm that uses environment's transition probabilities\n",
    "    \n",
    "    Args:\n",
    "        env: GridWorld environment\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "        \n",
    "    Returns:\n",
    "        V: Converged value function\n",
    "        policy: Optimal policy\n",
    "    \"\"\"\n",
    "    # TODO: Initialize Value \n",
    "    V = ...\n",
    "\n",
    "    # TODO: Initialize policy \n",
    "    policy = ...\n",
    "    \n",
    "    # Initialize terminal state values\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if env.is_terminal((i, j)):\n",
    "                V[i, j] = env.get_terminal_value((i, j))\n",
    "                policy[i, j] = -1\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(env.size):\n",
    "            for j in range(env.size):\n",
    "            \n",
    "                # TODO: Skip walls and terminal states\n",
    "                ... \n",
    "                \n",
    "                old_v = V[i, j]\n",
    "                state = (i, j)\n",
    "                \n",
    "                # Try all actions\n",
    "                action_values = []\n",
    "                for action in range(4):\n",
    "                    # Get transition probabilities for this state-action pair\n",
    "                    transitions = env.get_transition_probs(state, action)\n",
    "                    \n",
    "                    # Calculate expected value for this action\n",
    "                    expected_value = 0\n",
    "                    for next_state, prob in transitions:\n",
    "                        if env.is_terminal(next_state):\n",
    "                            # TODO: Update expected value when state is terminal\n",
    "                            expected_value ...\n",
    "                        else:\n",
    "                            # TODO: Update expected value when state is non terminal\n",
    "                            expected_value ...\n",
    "                            \n",
    "                    action_values.append(expected_value)\n",
    "                \n",
    "                # Update value and policy\n",
    "                V[i, j] = max(action_values)\n",
    "                policy[i, j] = np.argmax(action_values)\n",
    "                \n",
    "                delta = max(delta, abs(old_v - V[i, j]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "# Test value iteration\n",
    "create_interactive_visualization(GridWorld, value_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "**Policy Iteration** is a method for finding the optimal policy in a Markov Decision Process (MDP). It alternates between two steps:\n",
    "1. **Policy Evaluation**: Calculates the value function for the current policy.\n",
    "2. **Policy Improvement**: Updates the policy to be greedy with respect to the current value function, meaning it chooses actions that maximize expected returns.\n",
    "\n",
    "This process repeats until the policy stabilizes, meaning it no longer changes from one iteration to the next, indicating that we have found an optimal policy.\n",
    "\n",
    "### Policy Evaluation\n",
    "\n",
    "In the **policy evaluation** step, we calculate the value of each state $s$ under a given policy $\\pi$, denoted $V^\\pi(s)$. This value function $V^\\pi(s)$ represents the expected return starting from state $s$ and following policy $\\pi$ forever after. \n",
    "\n",
    "To compute $V^\\pi(s)$, we solve the **Bellman equation** for each state under the current policy:\n",
    "\n",
    "$V^\\pi(s) = \\sum_{s'} P(s'|s, \\pi(s)) \\left[ R(s, \\pi(s), s') + \\gamma V^\\pi(s') \\right]$\n",
    "\n",
    "Breaking down this equation:\n",
    "- $P(s'|s, \\pi(s))$: The probability of reaching next state $s'$ from state $s$ under the action prescribed by policy $\\pi$.\n",
    "- $R(s, \\pi(s), s')$: The immediate reward for transitioning from state $s$ to state $s'$ by taking the action specified by $\\pi$.\n",
    "- $\\gamma V^\\pi(s')$: The discounted value of the next state $s'$.\n",
    "- The **sum** over $s'$ takes into account all possible next states, weighted by their transition probabilities.\n",
    "\n",
    "### Policy Evaluation Pseudocode\n",
    "\n",
    "The following pseudocode outlines the policy evaluation process:\n",
    "\n",
    "**Input**: Policy $\\pi$, MDP $(S, A, P, R)$, discount factor $\\gamma$, tolerance $\\theta$  \n",
    "**Output**: Value function $V^\\pi$\n",
    "\n",
    "1. **Initialize**: Set $V(s) \\leftarrow 0$ for all $s \\in S$\n",
    "2. **Repeat**:\n",
    "    - Set $\\Delta \\leftarrow 0$ (to track the maximum change in values for this iteration)\n",
    "    - **For each** state $s \\in S$:\n",
    "        - Store the current value: $v \\leftarrow V(s)$\n",
    "        - Update the value function according to the Bellman equation:\n",
    "          $V(s) \\leftarrow \\sum_{s'} P(s'|s, \\pi(s)) \\left[ R(s, \\pi(s), s') + \\gamma V(s') \\right]$\n",
    "        - Update $\\Delta$: $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$\n",
    "3. **Until** $\\Delta < \\theta$ (indicating that the values have converged within the specified tolerance)\n",
    "4. **Return** $V$\n",
    "\n",
    "This policy evaluation step provides us with the value function $V^\\pi$ for the current policy $\\pi$. This process will be used in the next step, **Policy Improvement**, to see if we can improve the policy based on the computed value function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Compute the expected value in both cases where state is terminal, and when it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, V, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Iteratively evaluate a policy using environment's transition probabilities\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(env.size):\n",
    "            for j in range(env.size):\n",
    "                if env.grid[i, j] == 1 or env.is_terminal((i, j)):\n",
    "                    continue\n",
    "                    \n",
    "                old_v = V[i, j]\n",
    "                state = (i, j)\n",
    "                action = policy[i, j]\n",
    "                \n",
    "                # Get transition probabilities for the current policy\n",
    "                transitions = env.get_transition_probs(state, action)\n",
    "                \n",
    "                # Calculate expected value\n",
    "                expected_value = 0\n",
    "                for next_state, prob in transitions:\n",
    "                    if env.is_terminal(next_state):\n",
    "                        ## TODO: Update expected value when state is terminal\n",
    "                        expected_value...\n",
    "                    else:\n",
    "                        ## TODO: Update expected value when state is non terminal\n",
    "                        expected_value...\n",
    "                \n",
    "                V[i, j] = expected_value\n",
    "                delta = max(delta, abs(old_v - V[i, j]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "\n",
    "In the **Policy Improvement** step, we take the current value function $V^\\pi$ (calculated in the policy evaluation step) and update our policy to make it **greedy** with respect to this value function. This means that, for each state $s$, we select the action that maximizes the expected reward based on $V^\\pi$.\n",
    "\n",
    "The new policy, $\\pi'$, is defined by:\n",
    "\n",
    "$\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V^\\pi(s') \\right]$\n",
    "\n",
    "This equation selects, for each state $s$, the action $a$ that yields the highest expected return according to the current value function $V^\\pi$. By doing this, the policy $\\pi'$ is guaranteed to perform at least as well as $\\pi$, and if there is an improvement to be made, $\\pi'$ will be strictly better than $\\pi$. This result is known as the **Policy Improvement Theorem**.\n",
    "\n",
    "If the policy no longer changes between iterations (i.e., it becomes stable), then we know that the policy is **optimal**.\n",
    "\n",
    "### Policy Improvement Pseudocode\n",
    "\n",
    "The pseudocode below outlines the steps for policy improvement:\n",
    "\n",
    "**Input**: Value function $V$, MDP $(S, A, P, R)$, discount factor $\\gamma$  \n",
    "**Output**: Improved policy $\\pi$, stability flag\n",
    "\n",
    "1. **Initialize**: Set $\\text{policy\\_stable} \\leftarrow \\text{True}$\n",
    "2. **For each** state $s \\in S$:\n",
    "    - Set $\\text{old\\_action} \\leftarrow \\pi(s)$ (store the current action in the policy for state $s$)\n",
    "    - Update the policy to be greedy with respect to $V$:\n",
    "      $\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V(s') \\right]$\n",
    "    - **If** the action for $s$ has changed ($\\text{old\\_action} \\neq \\pi(s)$):\n",
    "        - Set $\\text{policy\\_stable} \\leftarrow \\text{False}$ (indicating that further policy improvement is possible)\n",
    "3. **Return** the updated policy $\\pi$ and the stability flag $\\text{policy\\_stable}$\n",
    "\n",
    "If $\\text{policy\\_stable}$ is `True` after an iteration, this means that no further changes were made to the policy, and we have found the optimal policy $\\pi^*$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the expected value in both cases where state is terminal, and when it is not.\n",
    "- Update Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, policy, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Make policy greedy with respect to current value function using transition probabilities\n",
    "    \"\"\"\n",
    "    policy_stable = True\n",
    "    \n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if env.grid[i, j] == 1 or env.is_terminal((i, j)):\n",
    "                continue\n",
    "            \n",
    "            old_action = policy[i, j]\n",
    "            state = (i, j)\n",
    "            \n",
    "            # Try all actions\n",
    "            action_values = []\n",
    "            for action in range(4):\n",
    "                # Get transition probabilities\n",
    "                transitions = env.get_transition_probs(state, action)\n",
    "                \n",
    "                # Calculate expected value\n",
    "                expected_value = 0\n",
    "                for next_state, prob in transitions:\n",
    "                    if env.is_terminal(next_state):\n",
    "                        expected_value += prob * (env.step_cost + env.get_terminal_value(next_state))\n",
    "                    else:\n",
    "                        expected_value += prob * (env.step_cost + gamma * V[next_state])\n",
    "                \n",
    "                action_values.append(expected_value)\n",
    "            \n",
    "            # TODO: Update policy\n",
    "            policy[i, j]...\n",
    "            \n",
    "            if old_action != policy[i, j]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    return policy, policy_stable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Policy Iteration\n",
    "\n",
    "By combining **Policy Evaluation** and **Policy Improvement**, we create the full **Policy Iteration** algorithm. Policy iteration alternates between these two steps until the policy becomes stable, meaning that no further improvements can be made and an optimal policy is found. \n",
    "\n",
    "The algorithm works as follows:\n",
    "1. **Policy Evaluation**: Given the current policy $\\pi$, compute the value function $V^\\pi$.\n",
    "2. **Policy Improvement**: Update the policy to make it greedy with respect to $V^\\pi$. If the policy does not change during this step, we have found an optimal policy.\n",
    "\n",
    "### Policy Iteration Pseudocode\n",
    "\n",
    "The pseudocode below summarizes the steps in policy iteration.\n",
    "\n",
    "**Input**: MDP $(S, A, P, R)$, discount factor $\\gamma$, tolerance $\\theta$  \n",
    "**Output**: Optimal value function $V^*$, optimal policy $\\pi^*$\n",
    "\n",
    "1. **Initialize**: Set $\\pi(s)$ arbitrarily for all $s \\in S$.\n",
    "2. **Repeat**:\n",
    "    - **Policy Evaluation**: Compute the value function $V$ for the current policy $\\pi$ using $\\text{PolicyEvaluation}(\\pi, S, A, P, R, \\gamma, \\theta)$.\n",
    "    - **Policy Improvement**: Update the policy to be greedy with respect to $V$:\n",
    "      $\\pi, \\text{policy\\_stable} \\leftarrow \\text{PolicyImprovement}(V, S, A, P, R, \\gamma)$\n",
    "3. **Until** $\\text{policy\\_stable}$ is `True` (indicating that the policy has converged to an optimal policy).\n",
    "4. **Return** the optimal value function $V^*$ and the optimal policy $\\pi^*$.\n",
    "\n",
    "In each iteration, policy iteration improves both the value function and the policy. When the policy no longer changes between iterations, it has reached the optimal policy $\\pi^*$, and $V$ becomes the optimal value function $V^*$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do \n",
    "\n",
    "- Compute Policy Evaluation\n",
    "- Compute Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecb7dd900334ae9b4880bbd03f6f701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Grid:', options=(('Grid 1', 0), ('Grid 2', 1), ('Grid 3', 21)), style=Des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985ce8d3da4e433a9559badf92ffd333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def policy_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Policy Iteration algorithm using environment's transition probabilities\n",
    "    \"\"\"\n",
    "    # Initialize values and policy\n",
    "    V = np.zeros((env.size, env.size))\n",
    "    policy = np.zeros((env.size, env.size), dtype=int)\n",
    "    \n",
    "    # Initialize terminal states\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if env.is_terminal((i, j)):\n",
    "                V[i, j] = env.get_terminal_value((i, j))\n",
    "                policy[i, j] = -1\n",
    "    \n",
    "    while True:\n",
    "        # TODO: Compute policy evaluation\n",
    "        ... \n",
    "        \n",
    "        # TODO: Compute Policy improvement\n",
    "        ...\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "# Test policy iteration\n",
    "create_interactive_visualization(GridWorld, policy_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Model-based Methods\n",
    "\n",
    "Both **Value Iteration** and **Policy Iteration** are model-based methods, meaning they rely on complete knowledge of the environment's dynamics to find the optimal policy. Specifically, these methods require:\n",
    "1. **Transition probabilities** $P(s'|s, a)$: The probability of moving to state $s'$ when taking action $a$ in state $s$.\n",
    "2. **Reward function** $R(s, a, s')$: The immediate reward received after transitioning from state $s$ to state $s'$ by taking action $a$.\n",
    "\n",
    "However, in many real-world scenarios:\n",
    "- The **environment dynamics** (the transition probabilities and reward function) are **unknown** or too complex to model precisely.\n",
    "- The **state space** (all possible states) is too large to store or calculate all transition probabilities, making it impractical to apply these model-based methods.\n",
    "\n",
    "These limitations create challenges for model-based methods, as they become impractical or infeasible when we lack a complete model of the environment or face large and complex state spaces.\n",
    "\n",
    "## The Need for Model-free Learning\n",
    "\n",
    "The limitations of model-based methods motivate the development of **model-free methods**, which can learn optimal policies directly from experience, without needing prior knowledge of the environment's transition probabilities and reward function. Instead of computing expected values with known transitions, as in:\n",
    "\n",
    "$Q^*(s, a) = \\sum_{s'} P(s'|s, a) \\left[ R(s, a, s') + \\gamma \\max_{a'} Q^*(s', a') \\right]$\n",
    "\n",
    "we can estimate **Q-values** using samples of experience collected by interacting with the environment. Each sample of experience consists of a tuple $(s, a, r, s')$, where:\n",
    "- $s$ is the current state,\n",
    "- $a$ is the action taken,\n",
    "- $r$ is the reward received,\n",
    "- $s'$ is the next state.\n",
    "\n",
    "Using these samples, model-free methods can update Q-value estimates based on the actual transitions experienced, rather than relying on predefined probabilities.\n",
    "\n",
    "This approach leads us to **Q-learning**, a popular model-free algorithm that:\n",
    "1. **Learns** directly from interaction with the environment, using observed experiences instead of a predefined model.\n",
    "2. **Updates Q-value estimates** by adjusting them based on observed rewards and transitions.\n",
    "3. **Converges to optimal Q-values** over time, meaning it can find an optimal policy without needing any prior knowledge of the environment's dynamics.\n",
    "\n",
    "In this way, model-free methods, such as Q-learning, provide a flexible and powerful approach to reinforcement learning, allowing agents to learn optimal behaviors in environments where model-based methods would be impractical or impossible to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning \n",
    "\n",
    "**Q-learning** is a model-free reinforcement learning algorithm that learns the optimal action-value function, denoted by $Q^*$, directly from experience. Unlike model-based approaches, Q-learning does not need information about transition probabilities or rewards beforehand; it learns purely from interactions with the environment.\n",
    "\n",
    "### Algorithm Description\n",
    "\n",
    "Q-learning maintains an action-value table, $Q(s, a)$, which stores estimates of the expected return (cumulative reward) for each state-action pair. The values in this table are updated using experience tuples $(s, a, r, s')$, where:\n",
    "- $s$ is the current state,\n",
    "- $a$ is the action taken in state $s$,\n",
    "- $r$ is the reward received after taking action $a$,\n",
    "- $s'$ is the resulting state after taking action $a$.\n",
    "\n",
    "The **Q-learning update rule** for each experience tuple is:\n",
    "\n",
    "$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the **learning rate**, which controls how much of the new information (target value) we consider in each update.\n",
    "- $\\gamma$ is the **discount factor**, which determines the importance of future rewards.\n",
    "- $r + \\gamma \\max_{a'} Q(s', a')$ is the **target value**—it represents the expected return for taking action $a$ in state $s$ and then following the best actions from $s'$ onward.\n",
    "- $r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$ is known as the **temporal difference error**—it measures the difference between the current estimate and the observed outcome.\n",
    "\n",
    "**Implementation Note**: This update rule can be algebraically rearranged to an equivalent form that's often used in code:\n",
    "\n",
    "$Q(s, a) \\leftarrow (1 - \\alpha)Q(s, a) + \\alpha(r + \\gamma \\max_{a'} Q(s', a'))$\n",
    "\n",
    "Both forms are mathematically identical. The second form can be derived from the first by factoring out $Q(s, a)$ and combining like terms. In practice, the second form is often preferred for implementation as it more directly expresses how to combine the old Q-value with the new target value using the learning rate $\\alpha$.\n",
    "\n",
    "### Action Selection: $\\epsilon$-greedy Policy\n",
    "\n",
    "To ensure the agent explores different actions and states while learning, Q-learning typically uses an **$\\epsilon$-greedy policy**:\n",
    "- With probability $1 - \\epsilon$, the agent **exploits** its current knowledge by selecting the action $a = \\arg\\max_a Q(s, a)$ (i.e., the action with the highest Q-value in state $s$).\n",
    "- With probability $\\epsilon$, the agent **explores** by choosing a random action, allowing it to gather new information.\n",
    "\n",
    "The exploration rate $\\epsilon$ can start high and decrease over time, allowing the agent to explore initially and then gradually focus on exploitation as it learns the optimal policy.\n",
    "\n",
    "### Q-Learning Pseudocode\n",
    "\n",
    "The following pseudocode summarizes the steps in the Q-learning algorithm.\n",
    "\n",
    "**Input**: Learning rate $\\alpha$, discount factor $\\gamma$, exploration rate $\\epsilon$  \n",
    "**Output**: Q-function $Q$, policy $\\pi$\n",
    "\n",
    "1. **Initialize** $Q(s, a) \\leftarrow 0$ for all $s \\in S$, $a \\in A$\n",
    "2. **For each episode**:\n",
    "    - Initialize the starting state $s$\n",
    "    - **While** $s$ is not a terminal state:\n",
    "        - **If** `random() < ε`:\n",
    "            - Select a random action $a$\n",
    "        - **Else**:\n",
    "            - Select action $a \\leftarrow \\arg\\max\\limits_{a'} Q(s, a')$\n",
    "        - Take action $a$, observe reward $r$ and next state $s'$\n",
    "        - Update Q-value:\n",
    "          $Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$\n",
    "        - Set $s \\leftarrow s'$\n",
    "3. **Extract policy**: $\\pi(s) \\leftarrow \\arg\\max\\limits_a Q(s, a)$ for all $s \\in S$\n",
    "\n",
    "After enough episodes, the Q-values converge, meaning $Q(s, a)$ approximates the optimal action-value function $Q^*(s, a)$, and the extracted policy $\\pi(s)$ becomes the optimal policy for the environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Implement the epsilon greedy action selection\n",
    "- Implement Q-Learning Update Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon):\n",
    "    \"\"\"Epsilon-greedy policy for action selection\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    return np.argmax(Q[state[0], state[1]])\n",
    "\n",
    "def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm with fixed exploration rate.\n",
    "    \n",
    "    Args:\n",
    "        env: GridWorld environment\n",
    "        episodes: Number of training episodes\n",
    "        alpha: Learning rate\n",
    "        gamma: Discount factor\n",
    "        epsilon: Fixed exploration rate\n",
    "    \"\"\"\n",
    "    # Initialize Q-values\n",
    "    Q = np.zeros((env.size, env.size, 4))\n",
    "    \n",
    "    # Initialize walls with -inf\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if env.grid[i, j] == 1:\n",
    "                Q[i, j].fill(-np.inf)\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # Training loop with progress bar\n",
    "    for episode in tqdm(range(episodes), desc='Training Progress'):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "\n",
    "            # TODO: Epsilon-greedy action selection\n",
    "            if ...\n",
    "                ...\n",
    "            else ...\n",
    "                ...\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Q-learning update\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                next_max_q = np.max(Q[next_state[0], next_state[1]])\n",
    "                target = reward + gamma * next_max_q\n",
    "                \n",
    "            # TODO: Update Q-value with learning rate alpha\n",
    "            Q[state[0], state[1], action] = ...\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Store metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    # Extract final policy and value function\n",
    "    policy = np.argmax(Q, axis=2)\n",
    "    V = np.max(Q, axis=2)\n",
    "    \n",
    "    # Set terminal state policies and values\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if env.grid[i, j] in [2, -1]:  # Goal or pit\n",
    "                V[i, j] = env.get_terminal_value((i, j))\n",
    "                policy[i, j] = -1\n",
    "            elif env.grid[i, j] == 1:  # Wall\n",
    "                policy[i, j] = -1\n",
    "    \n",
    "    return Q, episode_rewards, V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5f862d902e401ea2ea1300753c6939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Grid:', options=(('Grid 1', 0), ('Grid 2', 1), ('Grid 3', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8487224e1b904ae6bfc97c381d321093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_qlearning_visualization(GridWorld, q_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
