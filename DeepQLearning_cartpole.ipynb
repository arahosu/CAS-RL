{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Q-Learning Tutorial\n",
        "In the last tutorial you saw how we implemented the Q-Learning algorithm on a simple environment - the Girdworld! In gridworld, the number of possible states in the environment is small (state space $=$ height $\\times$ width), allowing the agent to visit all states when deriving the best Q-value function. In a given case like this, simple Q-learning works great and we can build an adequate RL model.\n",
        "\n",
        "Consider now a continuous environment which has a possibly infinite state-space, like an Atari video game such as Pong or SuperMarioBros. Does Q-learning work the same? \n",
        "\n",
        "The answer is no. A fundamental flaw of the traditional Q-learning paradigm is its need to store the unique Q-function state-action pair for each state in a lookup-table. For an environment with infinite state-space, you would need to store infinite Q-function state-action pairs in an infinite table. This is not possible. To address this issue we can choose to model the Q-function as a deep learning models to provide us with state-action pairs.\n",
        "\n",
        "We will have to make some adjustments to our algorithm first. The new Deep Q-Learning (DQN) algorithm is as follows:\n",
        "\n",
        "### Deep Q-Learning (DQN) Algorithm\n",
        "\n",
        "To accomodate our new continuous space setup, we introduce the replay buffer - where old memories of state-action pairs are sampled along with their rewards to train the model. We will model our Q-function as a feedforward neural network parameterized by parameters $\\Theta$ which are randomly initialized and trained using backpropogation using the mean squared error (MSE) loss funciton.\n",
        "\n",
        "1. We initialize the agent with the following\n",
        "    - A replay buffer of a given size limit. Newest entries to the buffer will overide the oldest.\n",
        "    - An action-value Q-function $Q$ with random parameters $\\Theta$. \n",
        "    - A target action-value Q-function $\\hat{Q}$ with parameters $\\hat{\\Theta} = \\Theta$\n",
        "2. Start the environment and allow the agent to interact with the environment and update/learn $\\Theta$ at the same time. The goal is to learn an optimal set of parameters $\\Theta$ during this interaction. \n",
        "    - During this interaction, the agent will fail many many times. So we decompose this interaction into a series of episodes where each episode is a session started from a default starting state and continues until the agent reaches a terminal state.\n",
        "    - In our environment, this terminal state is either is one of two options 1) the agent drops the pole, or 2) the agent succesfully holds the pole for 1000 timesteps (around 20 seconds).\n",
        "    - We train our model for a specified number of episodes or until sufficient successful terminal states has been achieved.\n",
        "3. Outer loop: We loop through episodes until the stopping conditions above\n",
        "    - Reset the environment and actor\n",
        "    - Loop through episode one timestep at a time\n",
        "        \n",
        "    - Inner loop: Take a step in the environment and get a new state $S_t$\n",
        "        - Actor decides on an action\n",
        "            - With probablility $\\varepsilon \\in [0,1]$ make a random action. With probability $1-\\varepsilon$ take an $a_t$ action given by $Q(S_t,\\Theta)$.\n",
        "        - Actor performs the action $a_t_ and observes the reward $R_t$ and next state $S_{t+1}$\n",
        "        - Store $S_t$, $a_t$, $R_t$, and $S_{t+1}$ in the replay buffer.\n",
        "        - Perform the replay step to train and update the target model $\\hat{Q}$. This step is crucial to learning the model and is explained in more detail next.\n",
        "\n",
        "#### The Replay Step\n",
        "To train the agent we allow it to interact with the environment. At each timestep of the game the agent is given the vectorized state of the environment and then required to take an action. With a chance of epsilon, the agent takes a random action. With a chance of 1-epsilon, the feedforward neural network chooses the action.\n",
        "\n",
        "To train this model, it is more efficient to replay past events, compare their success outcomes, and update the model parameters using backpropogation to converge to a stable solution.\n",
        "\n",
        "At each timestep, we take a random sample of past non-consecutive events which are stored in a replay buffer (the self.memory queue). Each time the agent takes a step in the evironment the current state, chosen action, observed reward and next state are saved in the buffer. Given we have a batch size of 128 and a replay buffer of size 2000, we take a random sample of 128 [$S_t$, $a_t$, $R_t$, $S_{t+1}$] pairs. We then compute the Q-function of each and compare predicted q-values to target q-values with the Mean Squared Error loss function. This loss is propagated backwards through the network to learn a better model.\n",
        "\n",
        "Remember, this is done for every step in the environment. As such, our model does not directly train on the exact last predicted action and state pair observed, but rather the last predicted state-action-reward pair is added to the queue replay buffer and we random sample 128 experienced steps from 2000 possible paris in this buffer to train.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cart-pole Environment\n",
        "## Lets put this into code!\n",
        "The following enviroment is a game where we try to balance a pole on top of a linear rail track. All we can decide is to either move the cart left or right. If the pole falls over, we lose! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependencies on google colab notebook\n",
        "These are installed on the notebook's server hosted at some google datacenter. Nothing is installed on your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh_cax-L8dWf",
        "outputId": "e02e5150-75c9-46a0-cc20-59c02c93120f"
      },
      "outputs": [],
      "source": [
        "#HIDE OUTPUT\n",
        "!pip install swig\n",
        "!pip install \"gymnasium[other]\"\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup the Deep Q-Learning agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sWb2ZEXWiId"
      },
      "outputs": [],
      "source": [
        "#Here we import some requried libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import cv2\n",
        "from IPython.display import Video\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al-P1ROZY5O1"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"You are running this on\", device)\n",
        "\n",
        "###                                                   ###\n",
        "#                                                       #\n",
        "#                  DEFINING THE AGENT                   #\n",
        "#                                                       #\n",
        "# In this section we define the deep q-learning agent.  #\n",
        "# We specify it's Q-function, modeled by a three layer. #\n",
        "# feedforward neural network. By inputing the state of. #\n",
        "# the environment into the Q-function, we get an predi- #\n",
        "# ction of the best action to take at that timestep.    #\n",
        "# We additionally specify a replay buffer which is used #\n",
        "# to train the model.                                   #\n",
        "###                                                   ###\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = 0.99  # discount factor\n",
        "        self.epsilon = 1.0  # initial exploration rate\n",
        "        self.epsilon_min = 0.001\n",
        "        self.epsilon_decay = 0.9995\n",
        "        self.batch_size = 128\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.device = device\n",
        "        self.m = nn.Softmax(dim=0)\n",
        "\n",
        "        # Initialize Q-network and target network\n",
        "        self.q_network = self.build_model().to(device)\n",
        "        self.target_network = self.build_model().to(device)\n",
        "        self.update_target_network()\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001) #We define the optimizer\n",
        "\n",
        "    def build_model(self):\n",
        "        # Our Q-function: a feedforward neural network model\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            #### TODO: complete the model!\n",
        "            # \n",
        "            # What goes here? How can we complete this feedforward model?\n",
        "            # \n",
        "            # ###\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.action_dim)\n",
        "        )\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            action = torch.rand(1).round()\n",
        "            action = action.int().item()\n",
        "            return action\n",
        "        with torch.no_grad():            \n",
        "            action = (self.q_network(torch.from_numpy(state).float().to(device))).cpu()\n",
        "            action = torch.argmax(action)\n",
        "            return action.item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self):\n",
        "        # The replay step explained earlier.\n",
        "        #\n",
        "        # At each timestep, we take a random sample of past non-consecutive events which are stored\n",
        "        # in a replay buffer (the self.memory queue). Each time the agent takes a step in the evironment\n",
        "        # the current state, chosen action, reward, and next observed state are saved in the buffer. \n",
        "        # Given we have a batch size of 128 and a replay buffer of size 2000, we take a random sample of\n",
        "        # 128 [state, action, reward, state+1] pairs. We then compute the Q-function of each and compare \n",
        "        # predicted q-values to target q-values with the Mean Squared Error loss function. This loss is propagated backwards through\n",
        "        # the network to learn a better model.\n",
        "        #\n",
        "        # Remember, this is done for every step in the environment. As such, our model does not directly \n",
        "        # train on the exact last predicted action and state pair observed, but rather the last predicted \n",
        "        # state-action-reward pair is added to the queue replay buffer and we random sample 128 experienced \n",
        "        # steps from 2000 possible paris in this buffer to train.\n",
        "\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        #### TODO: fix the batch sampling! Read the text above for clues..\n",
        "        #\n",
        "        # batch = random.sample(self.memory,    ???    )\n",
        "        #\n",
        "        ####\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        ## Storing arrays as torch tensors for efficiency\n",
        "        states = torch.tensor(np.asarray(states), dtype=torch.float32, device=device)\n",
        "        actions = torch.tensor(np.asarray(actions), dtype=torch.int64, device=device)\n",
        "        rewards = torch.tensor(np.asarray(rewards), dtype=torch.float32, device=device)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
        "        dones = torch.tensor(dones, dtype=torch.bool, device=device)\n",
        "\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1))\n",
        "        next_q_values = self.target_network(next_states).max(1)[0].unsqueeze(-1)\n",
        "        target_q_values = rewards.unsqueeze(-1) + self.gamma * next_q_values * ~dones.unsqueeze(-1)\n",
        "\n",
        "        #### TODO: fix the loss function!\n",
        "        #\n",
        "        # loss = nn.MSELoss()(  ???  ,  ???  )\n",
        "        #\n",
        "        ####\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Decay epsilon. Here we reduce the chance of random exploration (as opposed to\n",
        "        # neural network Q-function exploration) with each timestep. At the start of the\n",
        "        # training, we have a 99% chance of random action and 1% chance of model action\n",
        "        # By the end of the training, we have a 1% chance of random action and 99% chance of\n",
        "        # model action.\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            #### TODO: update the epsilon value of the next step!\n",
        "            #\n",
        "            # self.epsilon = ???\n",
        "            #\n",
        "            ####\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4sB4le1L6zL"
      },
      "outputs": [],
      "source": [
        "###                                                   ###\n",
        "#                                                       #\n",
        "#                    TRAINING AGENT                     #\n",
        "#                                                       #\n",
        "###                                                   ###\n",
        "def train_dqn(env, agent, episodes=1000, target_update=10):\n",
        "    total_rewards = []\n",
        "    max_num_steps = 1000\n",
        "    done_learning = False\n",
        "    max_reached = 0\n",
        "\n",
        "    # We loop through episodes until we reach a max episode. Each episode is a complete game run in which the agent \n",
        "    # interacts with its environment. When the episode ends, the environment and agent is reset to the default state.\n",
        "    for episode in range(episodes):\n",
        "        if done_learning:\n",
        "            break\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "\n",
        "            #### TODO: fix the step function! Maybe there is a hint for you elsewhere else in the code..\n",
        "            #\n",
        "            #   ???   , reward, done, _, _ = env.step(      ???     ) \n",
        "            #\n",
        "            ####\n",
        "\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "            agent.replay()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            #If we reach the maximum score (we survived to the last step of the simulation)\n",
        "            if step >= max_num_steps:\n",
        "                max_reached += 1\n",
        "                if max_reached >= 3: #lets reach the highest score three times before early stopping training\n",
        "                    done_learning = True\n",
        "            \n",
        "                total_rewards.append(total_reward)\n",
        "                break\n",
        "                \n",
        "            #If we terminated the game (due falling cart or moving off-screen)   \n",
        "            if done:\n",
        "                total_rewards.append(total_reward)\n",
        "                break\n",
        "\n",
        "        if episode % target_update == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode}, Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
        "\n",
        "    return total_rewards\n",
        "\n",
        "\n",
        "###                                                   ###\n",
        "#                                                       #\n",
        "#                    AGENT EVALUATION                   #\n",
        "#                                                       #\n",
        "###                                                   ###\n",
        "def evaluate_agent(env, agent, episodes=5, record=False, video_folder='videos'):\n",
        "    if record:\n",
        "        env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: True)\n",
        "    \n",
        "    max_num_steps = 1000\n",
        "    total_rewards = []\n",
        "    for episode in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        step = 0\n",
        "        done = False\n",
        "        print(\"running episode\", episode)\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "\n",
        "            next_state, reward, done, _, _ = env.step(action) # found me ;)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            done = False\n",
        "            if step > max_num_steps:\n",
        "                done = True\n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "    env.close()\n",
        "    return total_rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lets initialize the environment and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init the environment\n",
        "env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\n",
        "state_dim = np.prod(env.observation_space.shape)\n",
        "action_dim = np.prod(env.action_space.n)\n",
        "agent = DQNAgent(state_dim, action_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now, lets run the untrained model to see how it performs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LxisJymdo6lj",
        "outputId": "66aaae7b-bea4-4a8b-d98b-33fa981f2792"
      },
      "outputs": [],
      "source": [
        "# Evaluate the untrained agent and record video\n",
        "print(\"Evaluating untrained agent...\")\n",
        "evaluate_agent(env, agent, episodes=1, record=True, video_folder='videos/before_training')\n",
        "\n",
        "# SOMETIMES THIS CRASHES... RUN AGAIN. IT IS AN OCCASIONAL ERROR WITH THE VIDEO RECORDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Video(\"videos/before_training/rl-video-episode-0.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### That was bad.. lets train 💪 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "KYrIcrWrRTZc",
        "outputId": "15977659-8b94-4c71-bcab-422dc11cb8a7"
      },
      "outputs": [],
      "source": [
        "# Train the agent\n",
        "print(\"Training agent...\")\n",
        "total_rewards = train_dqn(env, agent, episodes=500, target_update=1)\n",
        "env.close()\n",
        "\n",
        "# Plot results of training\n",
        "plt.figure(figsize=(12,4), dpi= 100)\n",
        "plt.plot(total_rewards)\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.xlabel(\"Training episode\")\n",
        "plt.title(\"Evolution of episode reward during training\")\n",
        "plt.show()\n",
        "\n",
        "# If the model does not fully train within the given number of episodes, start again. If you reached the max number of \n",
        "# episodes, that means the model did not fully learn. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A reward of +1 is given for each frame the agent has correctly balanced the pole. A perfect score of 1000 is awarded to the agent if it has balanced it for 1000 frames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now lets look at the results after training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnPXJ_-CrBoz"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained agent and record video\n",
        "print(\"Evaluating trained agent...\")\n",
        "evaluate_agent(env, agent, episodes=1, record=True, video_folder='videos/after_training')\n",
        "\n",
        "# SOMETIMES THIS CRASHES... RUN AGAIN. IT IS AN OCCASIONAL ERROR WITH THE VIDEO RECORDING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Video(\"videos/after_training/rl-video-episode-0.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And if everything worked, we should have a balancing pole! \n",
        "\n",
        "If you had fun and would like more practice, feel free to adapt this notebook to another game. You can find more here https://gymnasium.farama.org/. A warning, some of these games take much longer to train and thus its recommended to try more advanced RL mehtods, such as Proximal Policy Optimization (PPO).  Enjoy :)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
